{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean train set, clean test set\n",
    "\n",
    "noise_type = ['insert0.003', 'insert0.006', 'insert0.009',\n",
    "            'moved0.003', 'moved0.006', 'moved0.009',\n",
    "            'replace0.003', 'replace0.006', 'replace0.009',\n",
    "            'rework0.003', 'rework0.006', 'rework0.009',\n",
    "            'skip0.003', 'skip0.006', 'skip0.009']\n",
    "\n",
    "for i in range(0, len(noise_type)):\n",
    "    # prefix 1~30 까지의 train set을 준비\n",
    "    train_df = pd.read_csv(concat_path + \"\\\\\" + noise_type[i] + '_BPIC15_1_' + 'target_ratio_10_concat.csv') # change (1)\n",
    "    train_df = train_df.set_index('Case')\n",
    "    \n",
    "    X_train = train_df.drop('label', axis=1)\n",
    "    X_train = X_train.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))    \n",
    "    y_train = train_df['label']\n",
    "\n",
    "    result_df = pd.DataFrame(columns=['Train 파일 이름', 'Test 파일 이름', 'Model', 'Accuracy Score', 'AUC Score'])\n",
    "\n",
    "    for m in range(1, 31):\n",
    "        file_pattern = os.path.join(washed_temp, '*.csv')\n",
    "        file_list = glob.glob(file_pattern)\n",
    "        lis = [s for s in file_list if 'BPIC15' in s and 'target_ratio_10' in s and noise_type[i] in s] # change (2)\n",
    "        lis = sorted(lis, key=lambda x: int(x.split('prefix')[1].split('_')[0]))\n",
    "        df = pd.read_csv(lis[m-1])\n",
    "\n",
    "        train_file_name =  noise_type[i] + '_BPIC15_1_' + 'target_ratio_10_concat.csv' # change (3)\n",
    "        test_file_name = \"prefix\" + str(m) + \"_\" + noise_type[i] + '_BPIC15_1_' + 'target_ratio_10_concat.csv' # change (4)\n",
    "\n",
    "        # 각 caseid 별 첫번째 event의 timestamp에 의한 train/test set 구분\n",
    "        # df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        df_head = df.groupby('Case').head(1).sort_values(by='Timestamp')\n",
    "\n",
    "        total_rows = len(df_head)\n",
    "        train_length = int(total_rows * 0.8)\n",
    "        train_data = df_head.iloc[:train_length]\n",
    "        test_data = df_head.iloc[train_length:]\n",
    "\n",
    "        encoder_test = Aggregate(case_id_col=['Case'], cat_cols=['Activity', 'label'], boolean=False)\n",
    "        encoder_test_df = encoder_test.fit_transform(df)\n",
    "        try:\n",
    "            encoder_test_df['label'] = encoder_test_df['label_deviant'].apply(lambda x: 0 if x > 0 else 1)\n",
    "        except KeyError:\n",
    "            encoder_test_df['label'] = encoder_test_df['label_regular'].apply(lambda x: 1 if x > 0 else 0)\n",
    "        try:\n",
    "            encoder_test_df.drop(['label_deviant', 'label_regular'], axis=1, inplace=True)\n",
    "        except KeyError:\n",
    "            try:\n",
    "                encoder_test_df.drop(['label_deviant'], axis=1, inplace=True)\n",
    "            except KeyError:\n",
    "                encoder_test_df.drop(['label_regular'], axis=1, inplace=True)\n",
    "\n",
    "        encoder_test_df = encoder_test_df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))    \n",
    "        encoder_test_df['set'] = np.where(encoder_test_df.index.isin(train_data['Case']), 'train', 'test')\n",
    "\n",
    "        train = encoder_test_df[encoder_test_df['set'] == 'train']\n",
    "        test = encoder_test_df[encoder_test_df['set'] == 'test']\n",
    "\n",
    "        X_test = test.drop(['label', 'set'], axis=1)\n",
    "        y_test = test['label']\n",
    "\n",
    "        # X_test의 컬럼명을 X_train의 컬럼명과 동일하게 변경, 단 X_test의 컬럼 수는 2개이고 X_train의 컬럼 수는 32개이므로 나머지 컬럼은 0으로 채움\n",
    "        missing_columns = list(set(X_train.columns) - set(X_test.columns))\n",
    "\n",
    "        # missing_columns\n",
    "        for col in missing_columns:\n",
    "            X_test[col] = 0\n",
    "\n",
    "        # 컬럼을 정렬하여 순서를 맞춤\n",
    "        X_test = X_test[X_train.columns]\n",
    "\n",
    "        models = ['Random Forest', 'XGB']\n",
    "        pipelines = [\n",
    "            Pipeline([('model', RandomForestClassifier())]),\n",
    "            Pipeline([('model', XGBClassifier())])\n",
    "        ]\n",
    "        param_grids = [\n",
    "            {\n",
    "                'model__n_estimators': [5, 10, 20],\n",
    "                'model__max_depth': [2, 4, 6, None],\n",
    "                'model__min_samples_split': [2, 5, 10],\n",
    "                'model__min_samples_leaf': [1, 2, 4],\n",
    "            },\n",
    "            {\n",
    "                'model__n_estimators': [5, 10, 20],\n",
    "                'model__max_depth': [2, 4, 6, None],\n",
    "                'model__learning_rate': [0.05, 0.1, 0.3],\n",
    "                'model__min_child_weight': [1, 3, 5]\n",
    "            },\n",
    "        ]\n",
    "        for model, pipeline, param_grid in zip(models, pipelines, param_grids):\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=2024)\n",
    "            grid_model = GridSearchCV(pipeline, param_grid=param_grid, cv=kf, scoring='accuracy')\n",
    "            if len(np.unique(y_train)) == 2:\n",
    "                grid_model.fit(X_train, y_train)\n",
    "                best_model = grid_model.best_estimator_\n",
    "                y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "            # AUC 계산을 시도하기 전에 y_test에 두 개의 클래스가 존재하는지 확인\n",
    "            if len(np.unique(y_test)) == 2:\n",
    "                auc = roc_auc_score(y_test, y_test_pred)\n",
    "                result_df = pd.concat([result_df, pd.DataFrame({'Train 파일 이름': [train_file_name], 'Test 파일 이름': [test_file_name],\n",
    "                                                                    'Model': [model], 'AUC Score': [auc]})], ignore_index=True)\n",
    "                \n",
    "                save_path = str(result_path + \"\\\\\" + noise_type[i] + '_BPIC15_1_' + 'target_ratio_10_concat.xlsx') # change (5)\n",
    "                result_df.to_excel(save_path, index=False)\n",
    "                print(result_df)\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping AUC calculation for {test_file_name} as only one class present in y_test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean train set, clean test set\n",
    "\n",
    "noise_type = ['insert0.003', 'insert0.006', 'insert0.009',\n",
    "            'moved0.003', 'moved0.006', 'moved0.009',\n",
    "            'replace0.003', 'replace0.006', 'replace0.009',\n",
    "            'rework0.003', 'rework0.006', 'rework0.009',\n",
    "            'skip0.003', 'skip0.006', 'skip0.009']\n",
    "\n",
    "for i in range(0, len(noise_type)):\n",
    "    # prefix 1~30 까지의 train set을 준비\n",
    "    train_df = pd.read_csv(concat_path + \"\\\\\" + noise_type[i] + '_BPIC15_1_' + 'target_ratio_20_concat.csv') # change (1)\n",
    "    train_df = train_df.set_index('Case')\n",
    "    \n",
    "    X_train = train_df.drop('label', axis=1)\n",
    "    X_train = X_train.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))    \n",
    "    y_train = train_df['label']\n",
    "\n",
    "    result_df = pd.DataFrame(columns=['Train 파일 이름', 'Test 파일 이름', 'Model', 'Accuracy Score', 'AUC Score'])\n",
    "\n",
    "    for m in range(1, 31):\n",
    "        file_pattern = os.path.join(washed_temp, '*.csv')\n",
    "        file_list = glob.glob(file_pattern)\n",
    "        lis = [s for s in file_list if 'BPIC15' in s and 'target_ratio_20' in s and noise_type[i] in s] # change (2)\n",
    "        lis = sorted(lis, key=lambda x: int(x.split('prefix')[1].split('_')[0]))\n",
    "        df = pd.read_csv(lis[m-1])\n",
    "\n",
    "        train_file_name =  noise_type[i] + '_BPIC15_1_' + 'target_ratio_20_concat.csv' # change (3)\n",
    "        test_file_name = \"prefix\" + str(m) + \"_\" + noise_type[i] + '_BPIC15_1_' + 'target_ratio_20_concat.csv' # change (4)\n",
    "\n",
    "        # 각 caseid 별 첫번째 event의 timestamp에 의한 train/test set 구분\n",
    "        # df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        df_head = df.groupby('Case').head(1).sort_values(by='Timestamp')\n",
    "\n",
    "        total_rows = len(df_head)\n",
    "        train_length = int(total_rows * 0.8)\n",
    "        train_data = df_head.iloc[:train_length]\n",
    "        test_data = df_head.iloc[train_length:]\n",
    "\n",
    "        encoder_test = Aggregate(case_id_col=['Case'], cat_cols=['Activity', 'label'], boolean=False)\n",
    "        encoder_test_df = encoder_test.fit_transform(df)\n",
    "        try:\n",
    "            encoder_test_df['label'] = encoder_test_df['label_deviant'].apply(lambda x: 0 if x > 0 else 1)\n",
    "        except KeyError:\n",
    "            encoder_test_df['label'] = encoder_test_df['label_regular'].apply(lambda x: 1 if x > 0 else 0)\n",
    "        try:\n",
    "            encoder_test_df.drop(['label_deviant', 'label_regular'], axis=1, inplace=True)\n",
    "        except KeyError:\n",
    "            try:\n",
    "                encoder_test_df.drop(['label_deviant'], axis=1, inplace=True)\n",
    "            except KeyError:\n",
    "                encoder_test_df.drop(['label_regular'], axis=1, inplace=True)\n",
    "\n",
    "        encoder_test_df = encoder_test_df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))    \n",
    "        encoder_test_df['set'] = np.where(encoder_test_df.index.isin(train_data['Case']), 'train', 'test')\n",
    "\n",
    "        train = encoder_test_df[encoder_test_df['set'] == 'train']\n",
    "        test = encoder_test_df[encoder_test_df['set'] == 'test']\n",
    "\n",
    "        X_test = test.drop(['label', 'set'], axis=1)\n",
    "        y_test = test['label']\n",
    "\n",
    "        # X_test의 컬럼명을 X_train의 컬럼명과 동일하게 변경, 단 X_test의 컬럼 수는 2개이고 X_train의 컬럼 수는 32개이므로 나머지 컬럼은 0으로 채움\n",
    "        missing_columns = list(set(X_train.columns) - set(X_test.columns))\n",
    "\n",
    "        # missing_columns\n",
    "        for col in missing_columns:\n",
    "            X_test[col] = 0\n",
    "\n",
    "        # 컬럼을 정렬하여 순서를 맞춤\n",
    "        X_test = X_test[X_train.columns]\n",
    "\n",
    "        models = ['Random Forest', 'XGB']\n",
    "        pipelines = [\n",
    "            Pipeline([('model', RandomForestClassifier())]),\n",
    "            Pipeline([('model', XGBClassifier())])\n",
    "        ]\n",
    "        param_grids = [\n",
    "            {\n",
    "                'model__n_estimators': [5, 10, 20],\n",
    "                'model__max_depth': [2, 4, 6, None],\n",
    "                'model__min_samples_split': [2, 5, 10],\n",
    "                'model__min_samples_leaf': [1, 2, 4],\n",
    "            },\n",
    "            {\n",
    "                'model__n_estimators': [5, 10, 20],\n",
    "                'model__max_depth': [2, 4, 6, None],\n",
    "                'model__learning_rate': [0.05, 0.1, 0.3],\n",
    "                'model__min_child_weight': [1, 3, 5]\n",
    "            },\n",
    "        ]\n",
    "        for model, pipeline, param_grid in zip(models, pipelines, param_grids):\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=2024)\n",
    "            grid_model = GridSearchCV(pipeline, param_grid=param_grid, cv=kf, scoring='accuracy')\n",
    "            if len(np.unique(y_train)) == 2:\n",
    "                grid_model.fit(X_train, y_train)\n",
    "                best_model = grid_model.best_estimator_\n",
    "                y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "            # AUC 계산을 시도하기 전에 y_test에 두 개의 클래스가 존재하는지 확인\n",
    "            if len(np.unique(y_test)) == 2:\n",
    "                auc = roc_auc_score(y_test, y_test_pred)\n",
    "                result_df = pd.concat([result_df, pd.DataFrame({'Train 파일 이름': [train_file_name], 'Test 파일 이름': [test_file_name],\n",
    "                                                                    'Model': [model], 'AUC Score': [auc]})], ignore_index=True)\n",
    "                \n",
    "                save_path = str(result_path + \"\\\\\" + noise_type[i] + '_BPIC15_1_' + 'target_ratio_20_concat.xlsx') # change (5)\n",
    "                result_df.to_excel(save_path, index=False)\n",
    "                print(result_df)\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping AUC calculation for {test_file_name} as only one class present in y_test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean train set, clean test set\n",
    "\n",
    "noise_type = ['insert0.003', 'insert0.006', 'insert0.009',\n",
    "            'moved0.003', 'moved0.006', 'moved0.009',\n",
    "            'replace0.003', 'replace0.006', 'replace0.009',\n",
    "            'rework0.003', 'rework0.006', 'rework0.009',\n",
    "            'skip0.003', 'skip0.006', 'skip0.009']\n",
    "\n",
    "for i in range(0, len(noise_type)):\n",
    "    # prefix 1~30 까지의 train set을 준비\n",
    "    train_df = pd.read_csv(concat_path + \"\\\\\" + noise_type[i] + '_BPIC15_1_' + 'target_ratio_30_concat.csv') # change (1)\n",
    "    train_df = train_df.set_index('Case')\n",
    "    \n",
    "    X_train = train_df.drop('label', axis=1)\n",
    "    X_train = X_train.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))    \n",
    "    y_train = train_df['label']\n",
    "\n",
    "    result_df = pd.DataFrame(columns=['Train 파일 이름', 'Test 파일 이름', 'Model', 'Accuracy Score', 'AUC Score'])\n",
    "\n",
    "    for m in range(1, 31):\n",
    "        file_pattern = os.path.join(washed_temp, '*.csv')\n",
    "        file_list = glob.glob(file_pattern)\n",
    "        lis = [s for s in file_list if 'BPIC15' in s and 'target_ratio_30' in s and noise_type[i] in s] # change (2)\n",
    "        lis = sorted(lis, key=lambda x: int(x.split('prefix')[1].split('_')[0]))\n",
    "        df = pd.read_csv(lis[m-1])\n",
    "\n",
    "        train_file_name =  noise_type[i] + '_BPIC15_1_' + 'target_ratio_30_concat.csv' # change (3)\n",
    "        test_file_name = \"prefix\" + str(m) + \"_\" + noise_type[i] + '_BPIC15_1_' + 'target_ratio_30_concat.csv' # change (4)\n",
    "\n",
    "        # 각 caseid 별 첫번째 event의 timestamp에 의한 train/test set 구분\n",
    "        # df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        df_head = df.groupby('Case').head(1).sort_values(by='Timestamp')\n",
    "\n",
    "        total_rows = len(df_head)\n",
    "        train_length = int(total_rows * 0.8)\n",
    "        train_data = df_head.iloc[:train_length]\n",
    "        test_data = df_head.iloc[train_length:]\n",
    "\n",
    "        encoder_test = Aggregate(case_id_col=['Case'], cat_cols=['Activity', 'label'], boolean=False)\n",
    "        encoder_test_df = encoder_test.fit_transform(df)\n",
    "        try:\n",
    "            encoder_test_df['label'] = encoder_test_df['label_deviant'].apply(lambda x: 0 if x > 0 else 1)\n",
    "        except KeyError:\n",
    "            encoder_test_df['label'] = encoder_test_df['label_regular'].apply(lambda x: 1 if x > 0 else 0)\n",
    "        try:\n",
    "            encoder_test_df.drop(['label_deviant', 'label_regular'], axis=1, inplace=True)\n",
    "        except KeyError:\n",
    "            try:\n",
    "                encoder_test_df.drop(['label_deviant'], axis=1, inplace=True)\n",
    "            except KeyError:\n",
    "                encoder_test_df.drop(['label_regular'], axis=1, inplace=True)\n",
    "\n",
    "        encoder_test_df = encoder_test_df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))    \n",
    "        encoder_test_df['set'] = np.where(encoder_test_df.index.isin(train_data['Case']), 'train', 'test')\n",
    "\n",
    "        train = encoder_test_df[encoder_test_df['set'] == 'train']\n",
    "        test = encoder_test_df[encoder_test_df['set'] == 'test']\n",
    "\n",
    "        X_test = test.drop(['label', 'set'], axis=1)\n",
    "        y_test = test['label']\n",
    "\n",
    "        # X_test의 컬럼명을 X_train의 컬럼명과 동일하게 변경, 단 X_test의 컬럼 수는 2개이고 X_train의 컬럼 수는 32개이므로 나머지 컬럼은 0으로 채움\n",
    "        missing_columns = list(set(X_train.columns) - set(X_test.columns))\n",
    "\n",
    "        # missing_columns\n",
    "        for col in missing_columns:\n",
    "            X_test[col] = 0\n",
    "\n",
    "        # 컬럼을 정렬하여 순서를 맞춤\n",
    "        X_test = X_test[X_train.columns]\n",
    "\n",
    "        models = ['Random Forest', 'XGB']\n",
    "        pipelines = [\n",
    "            Pipeline([('model', RandomForestClassifier())]),\n",
    "            Pipeline([('model', XGBClassifier())])\n",
    "        ]\n",
    "        param_grids = [\n",
    "            {\n",
    "                'model__n_estimators': [5, 10, 20],\n",
    "                'model__max_depth': [2, 4, 6, None],\n",
    "                'model__min_samples_split': [2, 5, 10],\n",
    "                'model__min_samples_leaf': [1, 2, 4],\n",
    "            },\n",
    "            {\n",
    "                'model__n_estimators': [5, 10, 20],\n",
    "                'model__max_depth': [2, 4, 6, None],\n",
    "                'model__learning_rate': [0.05, 0.1, 0.3],\n",
    "                'model__min_child_weight': [1, 3, 5]\n",
    "            },\n",
    "        ]\n",
    "        for model, pipeline, param_grid in zip(models, pipelines, param_grids):\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=2024)\n",
    "            grid_model = GridSearchCV(pipeline, param_grid=param_grid, cv=kf, scoring='accuracy')\n",
    "            if len(np.unique(y_train)) == 2:\n",
    "                grid_model.fit(X_train, y_train)\n",
    "                best_model = grid_model.best_estimator_\n",
    "                y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "            # AUC 계산을 시도하기 전에 y_test에 두 개의 클래스가 존재하는지 확인\n",
    "            if len(np.unique(y_test)) == 2:\n",
    "                auc = roc_auc_score(y_test, y_test_pred)\n",
    "                result_df = pd.concat([result_df, pd.DataFrame({'Train 파일 이름': [train_file_name], 'Test 파일 이름': [test_file_name],\n",
    "                                                                    'Model': [model], 'AUC Score': [auc]})], ignore_index=True)\n",
    "                \n",
    "                save_path = str(result_path + \"\\\\\" + noise_type[i] + '_BPIC15_1_' + 'target_ratio_30_concat.xlsx') # change (5)\n",
    "                result_df.to_excel(save_path, index=False)\n",
    "                print(result_df)\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping AUC calculation for {test_file_name} as only one class present in y_test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean train set, clean test set\n",
    "\n",
    "noise_type = ['insert0.003', 'insert0.006', 'insert0.009',\n",
    "            'moved0.003', 'moved0.006', 'moved0.009',\n",
    "            'replace0.003', 'replace0.006', 'replace0.009',\n",
    "            'rework0.003', 'rework0.006', 'rework0.009',\n",
    "            'skip0.003', 'skip0.006', 'skip0.009']\n",
    "\n",
    "for i in range(0, len(noise_type)):\n",
    "    # prefix 1~30 까지의 train set을 준비\n",
    "    train_df = pd.read_csv(concat_path + \"\\\\\" + noise_type[i] + '_BPIC15_1_' + 'target_ratio_40_concat.csv') # change (1)\n",
    "    train_df = train_df.set_index('Case')\n",
    "    \n",
    "    X_train = train_df.drop('label', axis=1)\n",
    "    X_train = X_train.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))    \n",
    "    y_train = train_df['label']\n",
    "\n",
    "    result_df = pd.DataFrame(columns=['Train 파일 이름', 'Test 파일 이름', 'Model', 'Accuracy Score', 'AUC Score'])\n",
    "\n",
    "    for m in range(1, 31):\n",
    "        file_pattern = os.path.join(washed_temp, '*.csv')\n",
    "        file_list = glob.glob(file_pattern)\n",
    "        lis = [s for s in file_list if 'BPIC15' in s and 'target_ratio_40' in s and noise_type[i] in s] # change (2)\n",
    "        lis = sorted(lis, key=lambda x: int(x.split('prefix')[1].split('_')[0]))\n",
    "        df = pd.read_csv(lis[m-1])\n",
    "\n",
    "        train_file_name =  noise_type[i] + '_BPIC15_1_' + 'target_ratio_40_concat.csv' # change (3)\n",
    "        test_file_name = \"prefix\" + str(m) + \"_\" + noise_type[i] + '_BPIC15_1_' + 'target_ratio_40_concat.csv' # change (4)\n",
    "\n",
    "        # 각 caseid 별 첫번째 event의 timestamp에 의한 train/test set 구분\n",
    "        # df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        df_head = df.groupby('Case').head(1).sort_values(by='Timestamp')\n",
    "\n",
    "        total_rows = len(df_head)\n",
    "        train_length = int(total_rows * 0.8)\n",
    "        train_data = df_head.iloc[:train_length]\n",
    "        test_data = df_head.iloc[train_length:]\n",
    "\n",
    "        encoder_test = Aggregate(case_id_col=['Case'], cat_cols=['Activity', 'label'], boolean=False)\n",
    "        encoder_test_df = encoder_test.fit_transform(df)\n",
    "        try:\n",
    "            encoder_test_df['label'] = encoder_test_df['label_deviant'].apply(lambda x: 0 if x > 0 else 1)\n",
    "        except KeyError:\n",
    "            encoder_test_df['label'] = encoder_test_df['label_regular'].apply(lambda x: 1 if x > 0 else 0)\n",
    "        try:\n",
    "            encoder_test_df.drop(['label_deviant', 'label_regular'], axis=1, inplace=True)\n",
    "        except KeyError:\n",
    "            try:\n",
    "                encoder_test_df.drop(['label_deviant'], axis=1, inplace=True)\n",
    "            except KeyError:\n",
    "                encoder_test_df.drop(['label_regular'], axis=1, inplace=True)\n",
    "\n",
    "        encoder_test_df = encoder_test_df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))    \n",
    "        encoder_test_df['set'] = np.where(encoder_test_df.index.isin(train_data['Case']), 'train', 'test')\n",
    "\n",
    "        train = encoder_test_df[encoder_test_df['set'] == 'train']\n",
    "        test = encoder_test_df[encoder_test_df['set'] == 'test']\n",
    "\n",
    "        X_test = test.drop(['label', 'set'], axis=1)\n",
    "        y_test = test['label']\n",
    "\n",
    "        # X_test의 컬럼명을 X_train의 컬럼명과 동일하게 변경, 단 X_test의 컬럼 수는 2개이고 X_train의 컬럼 수는 32개이므로 나머지 컬럼은 0으로 채움\n",
    "        missing_columns = list(set(X_train.columns) - set(X_test.columns))\n",
    "\n",
    "        # missing_columns\n",
    "        for col in missing_columns:\n",
    "            X_test[col] = 0\n",
    "\n",
    "        # 컬럼을 정렬하여 순서를 맞춤\n",
    "        X_test = X_test[X_train.columns]\n",
    "\n",
    "        models = ['Random Forest', 'XGB']\n",
    "        pipelines = [\n",
    "            Pipeline([('model', RandomForestClassifier())]),\n",
    "            Pipeline([('model', XGBClassifier())])\n",
    "        ]\n",
    "        param_grids = [\n",
    "            {\n",
    "                'model__n_estimators': [5, 10, 20],\n",
    "                'model__max_depth': [2, 4, 6, None],\n",
    "                'model__min_samples_split': [2, 5, 10],\n",
    "                'model__min_samples_leaf': [1, 2, 4],\n",
    "            },\n",
    "            {\n",
    "                'model__n_estimators': [5, 10, 20],\n",
    "                'model__max_depth': [2, 4, 6, None],\n",
    "                'model__learning_rate': [0.05, 0.1, 0.3],\n",
    "                'model__min_child_weight': [1, 3, 5]\n",
    "            },\n",
    "        ]\n",
    "        for model, pipeline, param_grid in zip(models, pipelines, param_grids):\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=2024)\n",
    "            grid_model = GridSearchCV(pipeline, param_grid=param_grid, cv=kf, scoring='accuracy')\n",
    "            if len(np.unique(y_train)) == 2:\n",
    "                grid_model.fit(X_train, y_train)\n",
    "                best_model = grid_model.best_estimator_\n",
    "                y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "            # AUC 계산을 시도하기 전에 y_test에 두 개의 클래스가 존재하는지 확인\n",
    "            if len(np.unique(y_test)) == 2:\n",
    "                auc = roc_auc_score(y_test, y_test_pred)\n",
    "                result_df = pd.concat([result_df, pd.DataFrame({'Train 파일 이름': [train_file_name], 'Test 파일 이름': [test_file_name],\n",
    "                                                                    'Model': [model], 'AUC Score': [auc]})], ignore_index=True)\n",
    "                \n",
    "                save_path = str(result_path + \"\\\\\" + noise_type[i] + '_BPIC15_1_' + 'target_ratio_40_concat.xlsx') # change (5)\n",
    "                result_df.to_excel(save_path, index=False)\n",
    "                print(result_df)\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping AUC calculation for {test_file_name} as only one class present in y_test.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
