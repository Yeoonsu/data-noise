{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import re\n",
    "import os\n",
    "import pm4py\n",
    "\n",
    "from Declare4Py.Encodings.Aggregate import Aggregate\n",
    "from Declare4Py.Encodings.IndexBased import IndexBased\n",
    "from Declare4Py.Encodings.Static import Static\n",
    "from Declare4Py.Encodings.PreviousState import PreviousState\n",
    "from Declare4Py.Encodings.LastState import LastState\n",
    "from Declare4Py.Encodings.Ngram import Ngram\n",
    "from Declare4Py.Encodings.Declare import Declare\n",
    "\n",
    "# Assuming that AccuracyScore class and preprocessing function are defined somewhere\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import permutations\n",
    "import json, os\n",
    "from collections import OrderedDict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from Declare4Py.Encodings.Aggregate import Aggregate\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_path = os.getcwd()\n",
    "input_path = os.sep.join([str(org_path), \"input\"])\n",
    "output_path = os.sep.join([str(org_path), \"output\"])\n",
    "train_temp = os.sep.join([str(org_path), \"train_temp\"])\n",
    "train_path = os.sep.join([str(org_path), \"train\"])\n",
    "temp_path = os.sep.join([str(org_path), \"temp\"])\n",
    "result_path = os.sep.join([str(org_path), \"result\"])\n",
    "concat_path = os.sep.join([str(org_path), \"concat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean concat\n",
    "\n",
    "file_list = os.listdir(temp_path)\n",
    "\n",
    "for i in range(0, 1):\n",
    "    csv_list = ['prefix' + str(i) + '_bpic2012_O_ACCEPTED-COMPLETE_clean.csv' for i in range(1, 31)] # change (1)\n",
    "\n",
    "    # csv_list를 prefix1부터 prefix30까지 정렬하기\n",
    "    csv_list = sorted(csv_list, key=lambda x: int(x.split('prefix')[1].split('_')[0]))\n",
    "\n",
    "    encoder_df = pd.DataFrame()  # 빈 데이터프레임 생성\n",
    "\n",
    "    for k in range(0, len(csv_list)):\n",
    "        df = pd.read_csv(temp_path + '/' + csv_list[k])\n",
    "\n",
    "        # Timestamp를 기준으로 앞 쪽 80%를 train 데이터로 선택\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        df_sorted = df.sort_values(by='Timestamp')\n",
    "        train_length = int(len(df_sorted) * 0.8)\n",
    "        train_data = df_sorted.iloc[:train_length]\n",
    "\n",
    "        # 인코딩 수행\n",
    "        encoder = Aggregate(case_id_col=['Case'], cat_cols=['Activity', 'label'], boolean=False)\n",
    "        encoder_df_temp = encoder.fit_transform(df)\n",
    "        encoder_df_temp['label'] = encoder_df_temp['label_deviant'].apply(lambda x: 0 if x > 0 else 1)\n",
    "        encoder_df_temp.drop(['label_deviant', 'label_regular'], axis=1, inplace=True)\n",
    "\n",
    "        # train 데이터와 test 데이터 분리\n",
    "        encoder_df_temp['set'] = np.where(encoder_df_temp.index.isin(train_data['Case']), 'train', 'test')\n",
    "        train = encoder_df_temp[encoder_df_temp['set'] == 'train']\n",
    "        train = train.drop(['set'], axis=1)\n",
    "\n",
    "        # train 데이터를 이어붙이기\n",
    "        encoder_df = pd.concat([encoder_df, train])\n",
    "\n",
    "        encoder_df = encoder_df.fillna(0)\n",
    "        encoder_df = encoder_df.astype(int)\n",
    "    \n",
    "        # 결과 저장\n",
    "        encoder_df.to_csv(concat_path + \"\\\\\" + 'clean_ACCEPTED_concat.csv', index=True) # change (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise concat\n",
    "\n",
    "file_list = os.listdir(temp_path)\n",
    "\n",
    "noise_type = ['insert0.003', 'insert0.006', 'insert0.009',\n",
    "            'moved0.003', 'moved0.006', 'moved0.009',\n",
    "            'replace0.003', 'replace0.006', 'replace0.009',\n",
    "            'rework0.003', 'rework0.006', 'rework0.009',\n",
    "            'skip0.003', 'skip0.006', 'skip0.009']\n",
    "\n",
    "for i in range(0, len(noise_type)):\n",
    "    csv_list = [s for s in file_list if 'ACCEPTED' in s and noise_type[i] in s and s.endswith('.csv')] # change (1)\n",
    "\n",
    "    # csv_list를 prefix1부터 prefix30까지 정렬하기\n",
    "    csv_list = sorted(csv_list, key=lambda x: int(x.split('prefix')[1].split('_')[0]))\n",
    "\n",
    "    encoder_df = pd.DataFrame()  # 빈 데이터프레임 생성\n",
    "\n",
    "    for k in range(0, len(csv_list)):\n",
    "        df = pd.read_csv(temp_path + '/' + csv_list[k])\n",
    "\n",
    "        # Timestamp를 기준으로 앞 쪽 80%를 train 데이터로 선택\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        df_sorted = df.sort_values(by='Timestamp')\n",
    "        train_length = int(len(df_sorted) * 0.8)\n",
    "        train_data = df_sorted.iloc[:train_length]\n",
    "\n",
    "        # 인코딩 수행\n",
    "        encoder = Aggregate(case_id_col=['Case'], cat_cols=['Activity', 'label'], boolean=False)\n",
    "        encoder_df_temp = encoder.fit_transform(df)\n",
    "        encoder_df_temp['label'] = encoder_df_temp['label_deviant'].apply(lambda x: 0 if x > 0 else 1)\n",
    "        encoder_df_temp.drop(['label_deviant', 'label_regular'], axis=1, inplace=True)\n",
    "\n",
    "        # train 데이터와 test 데이터 분리\n",
    "        encoder_df_temp['set'] = np.where(encoder_df_temp.index.isin(train_data['Case']), 'train', 'test')\n",
    "        train = encoder_df_temp[encoder_df_temp['set'] == 'train']\n",
    "        train = train.drop(['set'], axis=1)\n",
    "\n",
    "        # train 데이터를 이어붙이기\n",
    "        encoder_df = pd.concat([encoder_df, train])\n",
    "\n",
    "        encoder_df = encoder_df.fillna(0)\n",
    "        encoder_df = encoder_df.astype(int)\n",
    "    \n",
    "        # 결과 저장\n",
    "        encoder_df.to_csv(concat_path + \"\\\\\" + noise_type[i] +'_ACCEPTED_concat.csv', index=True) # change (2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
