{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- date : 2024-01-29\n",
    "- info : 15개의 BPIC15_1 anomaly pattern 데이터셋 만드는 파일\n",
    "- BPIC15_1 label 추가해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "org_path = os.getcwd()\n",
    "\n",
    "input_path = os.sep.join([str(org_path), \"input\"])\n",
    "temp_path =  os.sep.join([str(org_path), \"temp\"])\n",
    "output_path =  os.sep.join([str(org_path), \"output\"])\n",
    "seed =1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bpic2012_O_DECLINED-COMPLETE_clean.csv']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "from numpy import average\n",
    "import abnormal_patterns\n",
    "importlib.reload(abnormal_patterns)\n",
    "from abnormal_patterns import Abnorm_p\n",
    "\n",
    "\n",
    "# Show a list of data in 'input' folder\n",
    "file_list = os.listdir(input_path)\n",
    "csv_list = [s for s in file_list if 'bpic2012_O_DECLINED' in s]  # change '.csv'\n",
    "print(csv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bpic2012_O_DECLINED-COMPLETE_clean.csv']\n",
      "bpic2012_O_DECLINED-COMPLETE_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "from numpy import average\n",
    "import abnormal_patterns\n",
    "importlib.reload(abnormal_patterns)\n",
    "from abnormal_patterns import Abnorm_p\n",
    "\n",
    "\n",
    "# Show a list of data in 'input' folder\n",
    "print(csv_list)\n",
    "\n",
    "for dat in csv_list:\n",
    "    # [Page for data import]\n",
    "    print(dat)\n",
    "    event_log = pd.read_csv(input_path+ '\\\\' + dat)\n",
    "\n",
    "    # if dat == 'BPIC12.csv':\n",
    "    #     extracted_data = event_log[['Case', 'Activity', 'Timestamp']]\n",
    "    #     form = \"%Y/%m/%d %H:%M:%S.%f\"\n",
    "    \n",
    "    event_log = event_log.rename(columns={'Case ID':'Case', 'Activity':'Activity', 'Complete Timestamp':'Timestamp'})\n",
    "    extracted_data = event_log[['Case', 'Activity', 'Timestamp', 'label']]\n",
    "    form = \"%Y-%m-%d %H:%M:%S\"\n",
    "    \n",
    "    extracted_data.columns = [\"Case\", \"Activity\", \"Timestamp\", \"label\"] \n",
    "    extracted_data[\"Event\"] = list(range(0,len(event_log.index)))\n",
    "    cols = extracted_data.columns.tolist() \n",
    "    cols = cols[:1]+ cols[-1:] + cols[1:4] # Reorder columns\n",
    "    extracted_data = extracted_data[cols]\n",
    "\n",
    "    extracted_data = extracted_data.sort_values([\"Case\", \"Timestamp\", \"Activity\", \"label\"],ascending=[True, True, True, True]) # Reorder rows\n",
    "    extracted_data.to_csv(input_path + \"\\\\\" + dat , mode='w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpic2012_O_DECLINED-COMPLETE_clean.csv\n",
      "Started preprocessing to set the input of anomaly patterns\n",
      "Finished preprocessing (running time=0:00:03.424651)\n",
      "Started to inject anomaly patterns\n",
      "Finished to inject anomaly patterns (running time=0:00:01.253756)\n",
      "bpic2012_O_DECLINED-COMPLETE_clean.csv\n",
      "Started preprocessing to set the input of anomaly patterns\n",
      "Finished preprocessing (running time=0:00:03.310296)\n",
      "Started to inject anomaly patterns\n",
      "Finished to inject anomaly patterns (running time=0:00:01.477404)\n",
      "bpic2012_O_DECLINED-COMPLETE_clean.csv\n",
      "Started preprocessing to set the input of anomaly patterns\n",
      "Finished preprocessing (running time=0:00:03.285085)\n",
      "Started to inject anomaly patterns\n",
      "Finished to inject anomaly patterns (running time=0:00:01.272976)\n",
      "bpic2012_O_DECLINED-COMPLETE_clean.csv\n",
      "Started preprocessing to set the input of anomaly patterns\n",
      "Finished preprocessing (running time=0:00:04.962374)\n",
      "Started to inject anomaly patterns\n",
      "Finished to inject anomaly patterns (running time=0:00:02.181766)\n",
      "bpic2012_O_DECLINED-COMPLETE_clean.csv\n",
      "Started preprocessing to set the input of anomaly patterns\n",
      "Finished preprocessing (running time=0:00:03.313315)\n",
      "Started to inject anomaly patterns\n",
      "Finished to inject anomaly patterns (running time=0:00:01.150209)\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "from numpy import average\n",
    "import abnormal_patterns\n",
    "importlib.reload(abnormal_patterns)\n",
    "from abnormal_patterns import Abnorm_p\n",
    "\n",
    "\n",
    "# Show a list of data in 'input' folder\n",
    "# file_list = os.listdir(input_path)\n",
    "# csv_list = [s for s in file_list if 'BPIC15_1.csv' in s]\n",
    "# print(csv_list)\n",
    "\n",
    "typearray = [\"skip\",  \"insert\", \"rework\", \"moved\", \"replace\"]\n",
    "# rate = [0.003, 0.006, 0.009] # resource failure rate\n",
    "rate = [0.02]\n",
    "\n",
    "for typearra in typearray:\n",
    "    for rat in rate:\n",
    "        for dat in csv_list:\n",
    "            # [Page for data import]\n",
    "            print(dat)\n",
    "            event_log = pd.read_csv(input_path+ '\\\\' + dat)\n",
    "\n",
    "            # if dat == 'BPIC12.csv':\n",
    "            #     extracted_data = event_log[['Case', 'Activity', 'Timestamp']]\n",
    "            #     form = \"%Y/%m/%d %H:%M:%S.%f\"\n",
    "\n",
    "            event_log = event_log.rename(columns={'Case ID':'Case', 'Activity':'Activity', 'time:timestamp':'Timestamp'})\n",
    "            extracted_data = event_log[['Case', 'Activity', 'Timestamp', 'label']]\n",
    "            form = \"%Y-%m-%d %H:%M:%S\"\n",
    "            \n",
    "            extracted_data.columns = [\"Case\", \"Activity\", \"Timestamp\", \"label\"] \n",
    "            extracted_data[\"Event\"] = list(range(0,len(event_log.index)))\n",
    "            cols = extracted_data.columns.tolist() \n",
    "            cols = cols[:1]+ cols[-1:] + cols[1:4] # Reorder columns\n",
    "            extracted_data = extracted_data[cols]\n",
    "\n",
    "            extracted_data = extracted_data.sort_values([\"Case\", \"Timestamp\", \"Activity\", \"label\"],ascending=[True, True, True, True]) # Reorder rows\n",
    "            extracted_data['Timestamp'] = extracted_data['Timestamp'].str.slice(stop=19)\n",
    "            time = extracted_data['Timestamp'].apply(lambda x: dt.strptime(x, form))\n",
    "            unixtime = time.apply(lambda x: (x - dt(1970, 1, 1)).total_seconds())\n",
    "            extracted_data['Timestamp'] = time\n",
    "            extracted_data['unixtime'] = unixtime\n",
    "            extracted_data = extracted_data.dropna(subset=['Case'])\n",
    "            \n",
    "            ## [Page to set root-causes of resources]\n",
    "\n",
    "            # Just to show information of activity\n",
    "            params = {\n",
    "                'Case': 'count'\n",
    "            }\n",
    "            activitylist = extracted_data.groupby('Activity').agg(params).reset_index()\n",
    "\n",
    "            # First to generate resource groups\n",
    "            ngroup = 10  # parameter\n",
    "\n",
    "            k = 0\n",
    "            rl=list(np.repeat(\"act\",len(activitylist)))\n",
    "            for i in activitylist['Activity']:\n",
    "                k += 1\n",
    "                rl[k-1] = list([\"Resource_Group\" + str(np.random.randint(0, int(ngroup) ))])\n",
    "                \n",
    "            activitylist['Resource_Group'] = pd.DataFrame(rl)\n",
    "            activitylist = activitylist[['Activity' , 'Resource_Group']]\n",
    "            extracted_data4 = pd.merge(extracted_data, activitylist, on=\"Activity\")\n",
    "\n",
    "            # set a size of each group\n",
    "            groupsize = 10 # parameter\n",
    "            d = {'Resource_Group': [\"Resource_Group\" + str(i) for i in range(0, ngroup)],'Resource_Group_Size': range(0, groupsize)}\n",
    "            group_size = pd.DataFrame(data=d)\n",
    "            extracted_data4 = pd.merge(extracted_data4, group_size , on=\"Resource_Group\")\n",
    "\n",
    "            # generate resource\n",
    "            attach = [\"res_\" + str(np.random.randint(0,int(i)+1)) for i in extracted_data4[\"Resource_Group_Size\"]]\n",
    "            extracted_data4[\"attach\"] = attach\n",
    "            extracted_data4[\"Resource\"] = extracted_data4[['Resource_Group', 'attach']].apply(lambda x: \"_\".join(x) ,axis=1)\n",
    "            del extracted_data4['attach']\n",
    "            del extracted_data4['Resource_Group_Size']\n",
    "\n",
    "            # extract resource-perspective data\n",
    "            params = {\n",
    "                'Case': 'count',\n",
    "                'Activity': lambda x: ','.join(sorted(pd.Series.unique(x)))\n",
    "            }\n",
    "\n",
    "            extracted_data2 = extracted_data4\n",
    "            resourcelist2 = extracted_data4.groupby('Resource').agg(params).reset_index()\n",
    "            resourcelist2.columns = [\"Resource\", \"Frequency\", \"Activities\"]\n",
    "            resourcelist2 = resourcelist2.sort_values([\"Frequency\"], ascending=False)\n",
    "            cols = [\"Resource\", \"Frequency\",  \"Activities\"]\n",
    "            resourcelist2 = resourcelist2[cols]\n",
    "            resourcelist = resourcelist2\n",
    "\n",
    "            # set probability of resource failure\n",
    "            resourcelist2[\"Resource_failure_rate\"] = rat  # parameter\n",
    "            resourcelist3 = resourcelist2[[\"Resource\", \"Resource_failure_rate\"]]\n",
    "            extracted_data2 = pd.merge(extracted_data2, resourcelist3, on=\"Resource\")\n",
    "\n",
    "            # simulate resource failure\n",
    "            np.random.seed(seed)\n",
    "            PF = np.random.binomial(np.repeat(1, len(extracted_data2)), extracted_data2[\"Resource_failure_rate\"])\n",
    "            extracted_data2['Resource_Anomaly/Normal'] = PF\n",
    "            extracted_data3 = extracted_data2\n",
    "            \n",
    "\n",
    "            # if 'BPIC12.csv' in dat:\n",
    "            #     anomaly = Abnorm_p(extracted_data2).implement_resource(\n",
    "            #         # types=[\"skip\",  \"insert\", \"rework\", \"moved\", \"replace\"],\n",
    "            #         types=[typearra],\n",
    "            #         mag=[1])  # extracted_data2 = event log with pass/fail(resource)\n",
    "            # types= anomaly patterns, mag= weights\n",
    "                \n",
    "            anomaly = Abnorm_p(extracted_data2).implement_resource(\n",
    "                types=[typearra],\n",
    "                mag=[1]) \n",
    "            \n",
    "            # anomaly.to_csv(output_path + \"\\\\anomaly_\" +str(rate) + \"_\" + dat , mode='w', index=False)\n",
    "            \n",
    "            anomaly.to_csv(output_path + \"\\\\\"+ str(typearra) + str(rat) + \"_\" + dat, mode='w', index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chmod('c:\\\\Users\\\\yeon1\\\\OneDrive - UNIST\\\\바탕 화면\\\\processed_benchmark_event_logs\\\\temp\\\\data_with_parameter1.csv', 0o700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4685"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('output/insert0.03_bpic2012_O_ACCEPTED-COMPLETE_clean.csv')\n",
    "df['Case'].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Case\n",
       "173688    27\n",
       "173694    61\n",
       "173715    25\n",
       "173718    79\n",
       "173730    79\n",
       "          ..\n",
       "214148    21\n",
       "214220    31\n",
       "214226    41\n",
       "214250    25\n",
       "214268    40\n",
       "Length: 3118, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# anomaly\n",
    "df.groupby(\"Case\").filter(lambda x: x[\"Event\"].nunique() != x[\"Event\"].count()).groupby(\"Case\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Case\n",
       "173691    39\n",
       "173721    35\n",
       "173736    55\n",
       "173760    26\n",
       "173787    35\n",
       "          ..\n",
       "214058    31\n",
       "214061    40\n",
       "214238    30\n",
       "214277    28\n",
       "214361    28\n",
       "Length: 1567, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal\n",
    "df.groupby(\"Case\").filter(lambda x: x[\"Event\"].nunique() == x[\"Event\"].count()).groupby(\"Case\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b13726099ff4a9270d97cd5a303046c40236cea9d4b3d3acf7f22861afad882"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
