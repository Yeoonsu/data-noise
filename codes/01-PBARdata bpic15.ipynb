{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- date : 2024-01-29\n",
    "- info : 15개의 BPIC15_1 anomaly pattern 데이터셋 만드는 파일\n",
    "- BPIC15_1 label 추가해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "org_path = os.getcwd()\n",
    "\n",
    "input_path = os.sep.join([str(org_path), \"input\"])\n",
    "temp_path =  os.sep.join([str(org_path), \"temp\"])\n",
    "output_path =  os.sep.join([str(org_path), \"output\"])\n",
    "seed =1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BPIC15_5_f2_clean.csv']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "from numpy import average\n",
    "import abnormal_patterns\n",
    "importlib.reload(abnormal_patterns)\n",
    "from abnormal_patterns import Abnorm_p\n",
    "\n",
    "\n",
    "# Show a list of data in 'input' folder\n",
    "file_list = os.listdir(input_path)\n",
    "csv_list = [s for s in file_list if 'BPIC15_5_f2' in s]  # change '.csv'\n",
    "print(csv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BPIC15_5_f2_clean.csv']\n",
      "BPIC15_5_f2_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "from numpy import average\n",
    "import abnormal_patterns\n",
    "importlib.reload(abnormal_patterns)\n",
    "from abnormal_patterns import Abnorm_p\n",
    "\n",
    "\n",
    "# Show a list of data in 'input' folder\n",
    "print(csv_list)\n",
    "\n",
    "for dat in csv_list:\n",
    "    # [Page for data import]\n",
    "    print(dat)\n",
    "    event_log = pd.read_csv(input_path+ '\\\\' + dat)\n",
    "\n",
    "    # if dat == 'BPIC12.csv':\n",
    "    #     extracted_data = event_log[['Case', 'Activity', 'Timestamp']]\n",
    "    #     form = \"%Y/%m/%d %H:%M:%S.%f\"\n",
    "    \n",
    "    event_log = event_log.rename(columns={'Case ID':'Case', 'Activity':'Activity', 'time:timestamp':'Timestamp'})\n",
    "    extracted_data = event_log[['Case', 'Activity', 'Timestamp', 'label']]\n",
    "    form = \"%Y-%m-%d %H:%M:%S\"\n",
    "    \n",
    "    extracted_data.columns = [\"Case\", \"Activity\", \"Timestamp\", \"label\"] \n",
    "    extracted_data[\"Event\"] = list(range(0,len(event_log.index)))\n",
    "    cols = extracted_data.columns.tolist() \n",
    "    cols = cols[:1]+ cols[-1:] + cols[1:4] # Reorder columns\n",
    "    extracted_data = extracted_data[cols]\n",
    "\n",
    "    extracted_data = extracted_data.sort_values([\"Case\", \"Timestamp\", \"Activity\", \"label\"],ascending=[True, True, True, True]) # Reorder rows\n",
    "    extracted_data.to_csv(input_path + \"\\\\\" + dat , mode='w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Event</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3364103</td>\n",
       "      <td>0</td>\n",
       "      <td>01_HOOFD_010</td>\n",
       "      <td>2010-10-03 22:00:00</td>\n",
       "      <td>deviant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3364103</td>\n",
       "      <td>1</td>\n",
       "      <td>01_HOOFD_030_2</td>\n",
       "      <td>2010-10-12 22:00:00</td>\n",
       "      <td>deviant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3364103</td>\n",
       "      <td>2</td>\n",
       "      <td>01_HOOFD_065_2</td>\n",
       "      <td>2010-10-12 22:00:00</td>\n",
       "      <td>deviant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3364103</td>\n",
       "      <td>3</td>\n",
       "      <td>01_HOOFD_190_2</td>\n",
       "      <td>2010-10-12 22:00:00</td>\n",
       "      <td>deviant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3364103</td>\n",
       "      <td>4</td>\n",
       "      <td>08_AWB45_020_2</td>\n",
       "      <td>2010-10-12 22:00:00</td>\n",
       "      <td>deviant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54557</th>\n",
       "      <td>13494688</td>\n",
       "      <td>54557</td>\n",
       "      <td>01_HOOFD_065_0</td>\n",
       "      <td>2015-02-10 23:00:00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54558</th>\n",
       "      <td>13494688</td>\n",
       "      <td>54558</td>\n",
       "      <td>01_HOOFD_090</td>\n",
       "      <td>2015-02-10 23:00:00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54559</th>\n",
       "      <td>13494688</td>\n",
       "      <td>54559</td>\n",
       "      <td>02_DRZ_010</td>\n",
       "      <td>2015-02-10 23:00:00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54560</th>\n",
       "      <td>13494688</td>\n",
       "      <td>54560</td>\n",
       "      <td>04_BPT_005</td>\n",
       "      <td>2015-02-10 23:00:00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54561</th>\n",
       "      <td>13494688</td>\n",
       "      <td>54561</td>\n",
       "      <td>01_HOOFD_030_2</td>\n",
       "      <td>2015-02-11 13:52:03</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54562 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Case  Event        Activity            Timestamp    label\n",
       "0       3364103      0    01_HOOFD_010  2010-10-03 22:00:00  deviant\n",
       "1       3364103      1  01_HOOFD_030_2  2010-10-12 22:00:00  deviant\n",
       "2       3364103      2  01_HOOFD_065_2  2010-10-12 22:00:00  deviant\n",
       "3       3364103      3  01_HOOFD_190_2  2010-10-12 22:00:00  deviant\n",
       "4       3364103      4  08_AWB45_020_2  2010-10-12 22:00:00  deviant\n",
       "...         ...    ...             ...                  ...      ...\n",
       "54557  13494688  54557  01_HOOFD_065_0  2015-02-10 23:00:00  regular\n",
       "54558  13494688  54558    01_HOOFD_090  2015-02-10 23:00:00  regular\n",
       "54559  13494688  54559      02_DRZ_010  2015-02-10 23:00:00  regular\n",
       "54560  13494688  54560      04_BPT_005  2015-02-10 23:00:00  regular\n",
       "54561  13494688  54561  01_HOOFD_030_2  2015-02-11 13:52:03  regular\n",
       "\n",
       "[54562 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPIC15_5_f2_clean.csv\n",
      "Started preprocessing to set the input of anomaly patterns\n",
      "Finished preprocessing (running time=0:00:00.925860)\n",
      "Started to inject anomaly patterns\n",
      "Finished to inject anomaly patterns (running time=0:00:00.306635)\n",
      "BPIC15_5_f2_clean.csv\n",
      "Started preprocessing to set the input of anomaly patterns\n",
      "Finished preprocessing (running time=0:00:00.964434)\n",
      "Started to inject anomaly patterns\n",
      "Finished to inject anomaly patterns (running time=0:00:00.454343)\n",
      "BPIC15_5_f2_clean.csv\n",
      "Started preprocessing to set the input of anomaly patterns\n",
      "Finished preprocessing (running time=0:00:00.965941)\n",
      "Started to inject anomaly patterns\n",
      "Finished to inject anomaly patterns (running time=0:00:00.363933)\n",
      "BPIC15_5_f2_clean.csv\n",
      "Started preprocessing to set the input of anomaly patterns\n",
      "Finished preprocessing (running time=0:00:01.502050)\n",
      "Started to inject anomaly patterns\n",
      "Finished to inject anomaly patterns (running time=0:00:00.671148)\n",
      "BPIC15_5_f2_clean.csv\n",
      "Started preprocessing to set the input of anomaly patterns\n",
      "Finished preprocessing (running time=0:00:01.006744)\n",
      "Started to inject anomaly patterns\n",
      "Finished to inject anomaly patterns (running time=0:00:00.318352)\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "from numpy import average\n",
    "import abnormal_patterns\n",
    "importlib.reload(abnormal_patterns)\n",
    "from abnormal_patterns import Abnorm_p\n",
    "\n",
    "\n",
    "# Show a list of data in 'input' folder\n",
    "# file_list = os.listdir(input_path)\n",
    "# csv_list = [s for s in file_list if 'BPIC15_1.csv' in s]\n",
    "# print(csv_list)\n",
    "\n",
    "typearray = [\"skip\",  \"insert\", \"rework\", \"moved\", \"replace\"]\n",
    "rate = [0.02]\n",
    "\n",
    "for typearra in typearray:\n",
    "    for rat in rate:\n",
    "        for dat in csv_list:\n",
    "            # [Page for data import]\n",
    "            print(dat)\n",
    "            event_log = pd.read_csv(input_path+ '\\\\' + dat)\n",
    "\n",
    "            # if dat == 'BPIC12.csv':\n",
    "            #     extracted_data = event_log[['Case', 'Activity', 'Timestamp']]\n",
    "            #     form = \"%Y/%m/%d %H:%M:%S.%f\"\n",
    "\n",
    "            event_log = event_log.rename(columns={'Case':'Case', 'Activity':'Activity', 'time:timestamp':'Timestamp'})\n",
    "            extracted_data = event_log[['Case', 'Activity', 'Timestamp', 'label']]\n",
    "            form = \"%Y-%m-%d %H:%M:%S\"\n",
    "            \n",
    "            extracted_data.columns = [\"Case\", \"Activity\", \"Timestamp\", \"label\"] \n",
    "            extracted_data[\"Event\"] = list(range(0,len(event_log.index)))\n",
    "            cols = extracted_data.columns.tolist() \n",
    "            cols = cols[:1]+ cols[-1:] + cols[1:4] # Reorder columns\n",
    "            extracted_data = extracted_data[cols]\n",
    "\n",
    "            extracted_data = extracted_data.sort_values([\"Case\", \"Timestamp\", \"Activity\", \"label\"],ascending=[True, True, True, True]) # Reorder rows\n",
    "\n",
    "            form = '%Y-%m-%d %H:%M:%S'\n",
    "            # Timestamp 열을 문자열로 변환\n",
    "            extracted_data['Timestamp'] = extracted_data['Timestamp'].astype(str)\n",
    "            # strptime 함수 적용\n",
    "            # extracted_data['Timestamp'] = extracted_data['Timestamp'].apply(lambda x: dt.strptime(x, form))\n",
    "\n",
    "            time = extracted_data['Timestamp'].apply(lambda x: dt.strptime(x, form))\n",
    "            unixtime = time.apply(lambda x: (x - dt(1970, 1, 1)).total_seconds())\n",
    "            extracted_data['Timestamp'] = time\n",
    "            extracted_data['unixtime'] = unixtime\n",
    "            extracted_data = extracted_data.dropna(subset=['Case'])\n",
    "            \n",
    "            ## [Page to set root-causes of resources]\n",
    "\n",
    "            # Just to show information of activity\n",
    "            params = {\n",
    "                'Case': 'count'\n",
    "            }\n",
    "            activitylist = extracted_data.groupby('Activity').agg(params).reset_index()\n",
    "\n",
    "            # First to generate resource groups\n",
    "            ngroup = 10  # parameter\n",
    "\n",
    "            k = 0\n",
    "            rl=list(np.repeat(\"act\",len(activitylist)))\n",
    "            for i in activitylist['Activity']:\n",
    "                k += 1\n",
    "                rl[k-1] = list([\"Resource_Group\" + str(np.random.randint(0, int(ngroup) ))])\n",
    "                \n",
    "            activitylist['Resource_Group'] = pd.DataFrame(rl)\n",
    "            activitylist = activitylist[['Activity' , 'Resource_Group']]\n",
    "            extracted_data4 = pd.merge(extracted_data, activitylist, on=\"Activity\")\n",
    "\n",
    "            # set a size of each group\n",
    "            groupsize = 10 # parameter\n",
    "            d = {'Resource_Group': [\"Resource_Group\" + str(i) for i in range(0, ngroup)],'Resource_Group_Size': range(0, groupsize)}\n",
    "            group_size = pd.DataFrame(data=d)\n",
    "            extracted_data4 = pd.merge(extracted_data4, group_size , on=\"Resource_Group\")\n",
    "\n",
    "            # generate resource\n",
    "            attach = [\"res_\" + str(np.random.randint(0,int(i)+1)) for i in extracted_data4[\"Resource_Group_Size\"]]\n",
    "            extracted_data4[\"attach\"] = attach\n",
    "            extracted_data4[\"Resource\"] = extracted_data4[['Resource_Group', 'attach']].apply(lambda x: \"_\".join(x) ,axis=1)\n",
    "            del extracted_data4['attach']\n",
    "            del extracted_data4['Resource_Group_Size']\n",
    "\n",
    "            # extract resource-perspective data\n",
    "            params = {\n",
    "                'Case': 'count',\n",
    "                'Activity': lambda x: ','.join(sorted(pd.Series.unique(x)))\n",
    "            }\n",
    "\n",
    "            extracted_data2 = extracted_data4\n",
    "            resourcelist2 = extracted_data4.groupby('Resource').agg(params).reset_index()\n",
    "            resourcelist2.columns = [\"Resource\", \"Frequency\", \"Activities\"]\n",
    "            resourcelist2 = resourcelist2.sort_values([\"Frequency\"], ascending=False)\n",
    "            cols = [\"Resource\", \"Frequency\",  \"Activities\"]\n",
    "            resourcelist2 = resourcelist2[cols]\n",
    "            resourcelist = resourcelist2\n",
    "\n",
    "            # set probability of resource failure\n",
    "            resourcelist2[\"Resource_failure_rate\"] = rat  # parameter\n",
    "            resourcelist3 = resourcelist2[[\"Resource\", \"Resource_failure_rate\"]]\n",
    "            extracted_data2 = pd.merge(extracted_data2, resourcelist3, on=\"Resource\")\n",
    "\n",
    "            # simulate resource failure\n",
    "            np.random.seed(seed)\n",
    "            PF = np.random.binomial(np.repeat(1, len(extracted_data2)), extracted_data2[\"Resource_failure_rate\"])\n",
    "            extracted_data2['Resource_Anomaly/Normal'] = PF\n",
    "            extracted_data3 = extracted_data2\n",
    "            \n",
    "\n",
    "            # if 'BPIC12.csv' in dat:\n",
    "            #     anomaly = Abnorm_p(extracted_data2).implement_resource(\n",
    "            #         # types=[\"skip\",  \"insert\", \"rework\", \"moved\", \"replace\"],\n",
    "            #         types=[typearra],\n",
    "            #         mag=[1])  # extracted_data2 = event log with pass/fail(resource)\n",
    "                \n",
    "            anomaly = Abnorm_p(extracted_data2).implement_resource(\n",
    "                types=[typearra],\n",
    "                mag=[1]) \n",
    "            \n",
    "            # anomaly.to_csv(output_path + \"\\\\anomaly_\" +str(rate) + \"_\" + dat , mode='w', index=False)\n",
    "            \n",
    "            anomaly.to_csv(output_path + \"\\\\\"+ str(typearra) + str(rat) + \"_\" + dat , mode='w', index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chmod('c:\\\\Users\\\\yeon1\\\\data-noise-research2\\\\temp\\\\data_with_parameter1.csv', 0o700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b13726099ff4a9270d97cd5a303046c40236cea9d4b3d3acf7f22861afad882"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
